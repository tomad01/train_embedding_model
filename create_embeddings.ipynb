{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DToma/miniconda3/envs/pyml/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apihelper.dataloaders import get_finnish_data\n",
    "\n",
    "df = get_finnish_data(split=False)\n",
    "df[\"Description\"] = df[\"Description\"].apply(lambda x: \" \".join(x.split(\" \")[:50]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271758,\n",
       " DbName\n",
       " M42Production_211    271758\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"../data_exploration/data/dwp_cleaned_tickets.parquet\",\n",
    "    columns=[\"Title\", \"Description\", \"DbName\", \"ObjectID\"],\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "df = df[df['DbName'].isin(['M42Production_211'])]\n",
    "df = df[df[\"Title\"].notna()]\n",
    "df = df[df[\"Description\"].notna()]\n",
    "df = df[df[\"Title\"].apply(lambda x: len(str(x).strip().lower()) > 1 and len(str(x).strip().lower()) != \"test\")]\n",
    "df[\"Description\"] = df[\"Description\"].apply(lambda x: \" \".join(x.split(\" \")[:50]))\n",
    "# full text\n",
    "# df[\"FullText\"] = df[\"Title\"] + \" \" + df[\"Description\"]\n",
    "# del df[\"Title\"],df[\"Description\"]\n",
    "len(df), df[\"DbName\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/jina_embeddings\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True).to('mps')\n",
    "\n",
    "# When calling the `encode` function, you can choose a `task` based on the use case:\n",
    "# 'retrieval.query', 'retrieval.passage', 'separation', 'classification', 'text-matching'\n",
    "# Alternatively, you can choose not to pass a `task`, and no specific LoRA adapter will be used.\n",
    "foo = df.drop_duplicates(\"FullText\")\n",
    "del df\n",
    "embeddings = model.encode(foo[\"FullText\"].values.tolist(), task=\"classification\",batch_size=8,verbose=True)\n",
    "with open(f\"{data_dir}/desc_embeddings.npy\", \"wb\") as f:\n",
    "    np.save(f, embeddings)\n",
    "with open(f\"{data_dir}/desc_texts.npy\", \"wb\") as f:\n",
    "    np.save(f, foo[\"FullText\"].values)\n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence transformer embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# sentence_encoder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "# sentence_encoder = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\")\n",
    "# sentence_encoder = SentenceTransformer(\"dunzhang/stella_en_400M_v5\",trust_remote_code=True)\n",
    "sentence_encoder = SentenceTransformer(\n",
    "    \"dunzhang/stella_en_400M_v5\",\n",
    "    trust_remote_code=True,\n",
    "    device=\"cpu\",\n",
    "    config_kwargs={\"use_memory_efficient_attention\": False, \"unpad_inputs\": False}\n",
    ")\n",
    "# sentence_encoder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "# sentence_encoder = SentenceTransformer(\"distilbert-base-multilingual-cased\")\n",
    "# sentence_encoder = SentenceTransformer(\"google-bert/bert-base-multilingual-cased\")\n",
    "# distilbert-base-multilingual-cased,paraphrase-multilingual-MiniLM-L12-v2,paraphrase-multilingual-mpnet-base-v2\n",
    "data_dir = \"./data/stella_en_400M_v5\"\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/14308 [00:00<?, ?it/s]/Users/DToma/miniconda3/envs/pyml/lib/python3.11/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 14308/14308 [4:15:52<00:00,  1.07s/it] \n"
     ]
    }
   ],
   "source": [
    "foo = df.drop_duplicates(\"Description\")\n",
    "desc_embeddings = sentence_encoder.encode(foo[\"Description\"].values.tolist(), show_progress_bar=True, batch_size=16,normalize_embeddings=True)\n",
    "with open(f\"{data_dir}/desc_embeddings.npy\", \"wb\") as f:\n",
    "    np.save(f, desc_embeddings)\n",
    "with open(f\"{data_dir}/desc_texts.npy\", \"wb\") as f:\n",
    "    np.save(f, foo[\"Description\"].values)\n",
    "del desc_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10614/10614 [59:21<00:00,  2.98it/s] \n"
     ]
    }
   ],
   "source": [
    "foo = df.drop_duplicates(\"Title\")\n",
    "title_embeddings = sentence_encoder.encode(foo[\"Title\"].values.tolist(), show_progress_bar=True, batch_size=16,normalize_embeddings=True)\n",
    "with open(f\"{data_dir}/title_embeddings.npy\", \"wb\") as f:\n",
    "    np.save(f, title_embeddings)\n",
    "with open(f\"{data_dir}/title_texts.npy\", \"wb\") as f:\n",
    "    np.save(f, foo[\"Title\"].values)\n",
    "del title_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada3 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(119252, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "from apihelper.openai_embeddings import OpenAIAzureEmbeddings\n",
    "\n",
    "# use redis to store the embeddings\n",
    "import redis\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    \"../data_exploration/data/dwp_cleaned_tickets.parquet\",\n",
    "    columns=[\"Title\", \"Description\", \"DbName\", \"ObjectID\"],\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "df = df[df[\"Title\"].notna()]\n",
    "df = df[df[\"Description\"].notna()]\n",
    "df = df[df[\"Title\"].apply(lambda x: len(str(x).strip().lower()) != \"test\")]\n",
    "df = df[df[\"DbName\"] == \"M42Production_646\"]\n",
    "# df.sort_values(['ObjectID','DbName']).drop_duplicates(subset=['Description'],inplace=True,keep='first')\n",
    "# assert len(df) == len(df.drop_duplicates(subset=['ObjectID','DbName']))\n",
    "\n",
    "redis_conn = redis.Redis(\"localhost\", 6379, db=4)\n",
    "get_emb = OpenAIAzureEmbeddings()\n",
    "# delete all keys\n",
    "# redis_conn.flushdb()\n",
    "len(df),len(redis_conn.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119252/119252 [4:34:09<00:00,  7.25it/s]  \n"
     ]
    }
   ],
   "source": [
    "for string in tqdm(df[\"Title\"].values, total=len(df)):\n",
    "    if not string in redis_conn:\n",
    "        emb = get_emb.get_embedding([string])\n",
    "        redis_conn.set(string, emb[0].tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Description\"] = df[\"Description\"].apply(lambda x: \" \".join(x.split(\" \")[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119252/119252 [4:47:20<00:00,  6.92it/s]  \n"
     ]
    }
   ],
   "source": [
    "for string in tqdm(df[\"Description\"].values, total=len(df)):\n",
    "    if not string in redis_conn:\n",
    "        emb = get_emb.get_embedding([string])\n",
    "        redis_conn.set(string, emb[0].tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119252/119252 [00:03<00:00, 33592.93it/s]\n"
     ]
    }
   ],
   "source": [
    "recover = []\n",
    "for string in tqdm(df[\"Description\"].values, total=len(df)):\n",
    "    emb = np.frombuffer(redis_conn.get(string), dtype=np.float32)\n",
    "    recover.append((string, emb))\n",
    "texts, embeddings = zip(*recover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/M42Production_646_ada3_small\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{data_dir}/desc_embeddings.npy\", \"wb\") as f:\n",
    "    np.save(f, np.array(embeddings))\n",
    "with open(f\"{data_dir}/desc_texts.npy\", \"wb\") as f:\n",
    "    np.save(f, np.array(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119252/119252 [00:03<00:00, 34410.74it/s]\n"
     ]
    }
   ],
   "source": [
    "recover = []\n",
    "for string in tqdm(df[\"Title\"].values, total=len(df)):\n",
    "    emb = np.frombuffer(redis_conn.get(string), dtype=np.float32)\n",
    "    recover.append((string, emb))\n",
    "texts, embeddings = zip(*recover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_dir}/title_embeddings.npy\", \"wb\") as f:\n",
    "    np.save(f, np.array(embeddings))\n",
    "with open(f\"{data_dir}/title_texts.npy\", \"wb\") as f:\n",
    "    np.save(f, np.array(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
